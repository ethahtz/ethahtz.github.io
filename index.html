<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Home | Etha Tianze Hua</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <link rel="icon" href="./resources/exmachina2.png">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Etha (Tianze) Hua</h1>

        <p>First-year ScM in Computer Science <br> Brown University </p>

        <img src="./resources/pic2.jpg" width="60%" alt="Logo">

        <p style="font-size:12px"><br>Currently based in <b>Providence, Rhode Island</b><br> 
        
        
        tianze_hua [AT] brown.edu <br> </p>
          
        <p style="font-size:16px"><b><a href="./index.html">About</a></b></p>
  
        <!-- <p style="font-size:16px"><b><a href="https://ethahtz.github.io/publications/">Publications</a></b></p> -->

        <!-- <p style="font-size:16px"><b><a href="./resources/etha_tianze_hua.pdf">CV</a></b></p> -->

        <a href="https://www.linkedin.com/in/ethathua/">LinkedIn</a> | <a href="https://twitter.com/EthaHua">Twitter</a> | <a href="https://github.com/ethahtz">GitHub</a>

      </header>
      <section>
          <!-- Page Content starts here -->


          <h3>Research Interest</h3>

          <p>
            I am interested in natural language understanding and its relation with the symbol grounding problem. 
            I believe for us to attribute cognitive properties (m knows p; m understands English; m is conscious; etc.) to AI systems, two components are required: 
            (a) operationalzied definitions of those cognitive properties, and (b) sufficient knowledge of the inner workings of the AI systems of interest.
            While the first component has been the subject of lengthy debates in the realms of philosophy of mind and epistemology and may continue to be unresolved, 
            the second component offers a domain (interpretability) where concrete answers are achievable.
            
            <br><br>

            To deepen our understanding of the internal mechanisms in LLMs, I am currently investigating the inference-time representational modules within transformer models. 

            <br><br>

            Besides interpretability, I am also interested in experimenting and analyzing language understanding 
            through <a href="https://langsci-press.org/catalog/book/49">multi-agent interactions</a>.

          </p>

          <hr>

          <h3>Updates</h3>

          <ul>
            <li>August 2023: I attended the <a href="https://cmmrs.mpi-sws.org/">Cornell, Maryland, Max Planck Pre-doctoral Research School 2023</a> in Saarbr√ºcken, Germany.</li>
          </ul>


          <hr>

          <h3>Education</h3>
            <dl>

              <dt>2023-now</dt>
              <dd>Sc.M. Computer Science, <a href="https://www.brown.edu/">Brown University</a> <br> </dd>

              <dt>2019-2023</dt>  
              <dd>B.S. Computer Science &amp; Philosophy, <a href="https://www.tufts.edu/">Tufts University </a> <br> </dd>
            
            </dl>

          
          <!-- Page Content ends here   -->
      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
